{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udf1f Master SLM Notebook: From Scratch Long-Context Model\n",
                "\n",
                "This master notebook consolidates the entire pipeline for building a 300M parameter Small Language Model (SLM) trained **exclusively on pre-1986 knowledge**.\n",
                "\n",
                "### \ud83d\udee0\ufe0f Pipeline Stages:\n",
                "1. **\ud83d\udcca Data Exploration**: Analyze pre-1986 training streams.\n",
                "2. **\ud83d\udd24 Tokenizer Training**: Build a custom BPE tokenizer.\n",
                "3. **\ud83c\udfd7\ufe0f Model Architecture**: Implement RoPE, Block-Local Attention, and Transformer blocks from scratch.\n",
                "4. **\ud83d\ude80 Training Pipeline**: Execute the 3-phase curriculum learning (Pretrain \u2192 Context Extend \u2192 Fine-tune).\n",
                "5. **\ud83d\udd2c Evaluation**: Perform zero-shot reasoning tests."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 1. Setup & Configuration\n",
                "\n",
                "Initialize environment, imports, and reproducibility seeds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "import time\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, Tuple, List\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "# Install dependencies if missing (for Colab)\n",
                "try:\n",
                "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
                "except ImportError:\n",
                "    !pip install tokenizers\n",
                "    from tokenizers import Tokenizer\n",
                "    from tokenizers.models import BPE\n",
                "    from tokenizers.trainers import BpeTrainer\n",
                "    from tokenizers.pre_tokenizers import Whitespace\n",
                "\n",
                "# Device Setup\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# Reproducibility\n",
                "torch.manual_seed(42)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(42)\n",
                "\n",
                "# Path Configuration\n",
                "# Adjust these if running in a different environment\n",
                "DATA_DIR = Path(\"../data/pre1986_training_streams_v1_FINAL\")\n",
                "TOKENIZER_DIR = Path(\"tokenizer_out\")\n",
                "TOKENIZER_DIR.mkdir(exist_ok=True)\n",
                "CHECKPOINT_DIR = Path(\"checkpoints_out\")\n",
                "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
                "CONFIG_DIR = Path(\"configs_out\")\n",
                "CONFIG_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "print(\"\\u2713 Environment ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 2. Data Exploration\n",
                "\n",
                "Analyze the training streams to understand data scale and content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_file_stats(filepath):\n",
                "    if not filepath.exists(): \n",
                "        return None\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        content = f.read()\n",
                "    return {\n",
                "        'size_mb': filepath.stat().st_size / (1024 * 1024),\n",
                "        'words': len(content.split()),\n",
                "        'eos_counts': content.count('<EOS>')\n",
                "    }\n",
                "\n",
                "files = ['base_stream.txt', 'finetune_control.txt', \n",
                "         'finetune_nuclear.txt', 'finetune_reliability.txt']\n",
                "\n",
                "print(f\"Scanning {DATA_DIR}...\")\n",
                "stats = {}\n",
                "for fname in files:\n",
                "    fpath = DATA_DIR / fname\n",
                "    s = get_file_stats(fpath)\n",
                "    if s:\n",
                "        stats[fname] = s\n",
                "        print(f\"{fname}: {s['size_mb']:.2f} MB, {s['words']:,} words, {s['eos_counts']} docs\")\n",
                "    else:\n",
                "        print(f\"Warning: {fname} not found in {DATA_DIR}\")\n",
                "\n",
                "# Simple visualization\n",
                "if stats:\n",
                "    names = list(stats.keys())\n",
                "    sizes = [stats[n]['size_mb'] for n in names]\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    plt.bar(names, sizes, color=['#2ecc71', '#3498db', '#e74c3c', '#9b59b6'])\n",
                "    plt.title('Dataset Size Distribution')\n",
                "    plt.ylabel('Size (MB)')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 3. Tokenizer Training\n",
                "\n",
                "Train a BPE tokenizer on the base stream. We aim for a vocab size of 32,000."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tokenizers import Tokenizer\n",
                "from tokenizers.models import BPE\n",
                "from tokenizers.trainers import BpeTrainer\n",
                "from tokenizers.pre_tokenizers import Whitespace\n",
                "\n",
                "# 1. Initialize\n",
                "tokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
                "tokenizer.pre_tokenizer = Whitespace()\n",
                "\n",
                "# 2. Configure Trainer\n",
                "trainer = BpeTrainer(\n",
                "    vocab_size=32000,\n",
                "    min_frequency=2,\n",
                "    special_tokens=[\"<PAD>\", \"<UNK>\", \"<EOS>\", \"<BOS>\"]\n",
                ")\n",
                "\n",
                "# 3. Train on base_stream\n",
                "base_stream_path = DATA_DIR / \"base_stream.txt\"\n",
                "if base_stream_path.exists():\n",
                "    print(\"Training tokenizer on base_stream... (this may take a moment)\")\n",
                "    tokenizer.train([str(base_stream_path)], trainer)\n",
                "    \n",
                "    # Save\n",
                "    tokenizer_path = TOKENIZER_DIR / \"tokenizer.json\"\n",
                "    tokenizer.save(str(tokenizer_path))\n",
                "    print(f\"\\u2713 Tokenizer saved to {tokenizer_path} (Vocab: {tokenizer.get_vocab_size()})\")\n",
                "else:\n",
                "    print(\"\\u274c Base stream not found, cannot train tokenizer.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 4. Model Architecture\n",
                "\n",
                "Implementation of the transform decoder with RoPE and Block-Local Attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ModelConfig:\n",
                "    vocab_size: int = 32000\n",
                "    d_model: int = 1024\n",
                "    n_layers: int = 24\n",
                "    n_heads: int = 16\n",
                "    d_ff: int = 4096\n",
                "    max_seq_len: int = 2048\n",
                "    block_size: int = 512\n",
                "    use_block_local: bool = False\n",
                "    dropout: float = 0.1\n",
                "\n",
                "    @property\n",
                "    def head_dim(self):\n",
                "        return self.d_model // self.n_heads\n",
                "\n",
                "# Save Config Helper\n",
                "def save_config(config: ModelConfig, path: Path):\n",
                "    with open(path, 'w') as f:\n",
                "        json.dump(config.__dict__, f, indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RotaryEmbedding(nn.Module):\n",
                "    \"\"\"Rotary Position Embeddings (RoPE)\"\"\"\n",
                "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
                "        super().__init__()\n",
                "        self.dim = dim\n",
                "        self.max_seq_len = max_seq_len\n",
                "        self.base = base\n",
                "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
                "        self.register_buffer('inv_freq', inv_freq)\n",
                "        self._build_cache(max_seq_len)\n",
                "    \n",
                "    def _build_cache(self, seq_len: int):\n",
                "        positions = torch.arange(seq_len).float()\n",
                "        freqs = torch.outer(positions, self.inv_freq)\n",
                "        emb = torch.cat([freqs, freqs], dim=-1)\n",
                "        self.register_buffer('cos_cached', emb.cos())\n",
                "        self.register_buffer('sin_cached', emb.sin())\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, seq_len: int):\n",
                "        if seq_len > self.max_seq_len:\n",
                "            self._build_cache(seq_len)\n",
                "            self.max_seq_len = seq_len\n",
                "        return (\n",
                "            self.cos_cached[:seq_len].to(x.device),\n",
                "            self.sin_cached[:seq_len].to(x.device)\n",
                "        )\n",
                "\n",
                "def rotate_half(x: torch.Tensor):\n",
                "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
                "    return torch.cat([-x2, x1], dim=-1)\n",
                "\n",
                "def apply_rotary_emb(q, k, cos, sin):\n",
                "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
                "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
                "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
                "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
                "    return q_embed, k_embed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.n_heads = config.n_heads\n",
                "        self.head_dim = config.head_dim\n",
                "        self.d_model = config.d_model\n",
                "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.rotary = RotaryEmbedding(self.head_dim, config.max_seq_len)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
                "\n",
                "    def forward(self, x, mask=None):\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        \n",
                "        cos, sin = self.rotary(q, seq_len)\n",
                "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
                "        \n",
                "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
                "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        if mask is None:\n",
                "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
                "        attn = attn.masked_fill(mask, float('-inf'))\n",
                "        attn = F.softmax(attn, dim=-1)\n",
                "        attn = self.dropout(attn)\n",
                "        \n",
                "        out = torch.matmul(attn, v)\n",
                "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
                "        return self.out_proj(out)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BlockLocalAttention(nn.Module):\n",
                "    \"\"\"Sparse attention: attends to current + previous block\"\"\"\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.n_heads = config.n_heads\n",
                "        self.head_dim = config.head_dim\n",
                "        self.d_model = config.d_model\n",
                "        self.block_size = config.block_size\n",
                "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.rotary = RotaryEmbedding(self.head_dim, config.max_seq_len)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        block_size = self.block_size\n",
                "        pad_len = (block_size - seq_len % block_size) % block_size\n",
                "        if pad_len > 0: x = F.pad(x, (0, 0, 0, pad_len))\n",
                "        padded_len = x.shape[1]\n",
                "        n_blocks = padded_len // block_size\n",
                "        \n",
                "        q = self.q_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        k = self.k_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        v = self.v_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        \n",
                "        cos, sin = self.rotary(q, padded_len)\n",
                "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
                "        \n",
                "        q = q.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        k = k.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        v = v.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        \n",
                "        k_prev = F.pad(k, (0,0,0,0,0,0,1,0))[:, :-1]\n",
                "        v_prev = F.pad(v, (0,0,0,0,0,0,1,0))[:, :-1]\n",
                "        k_local = torch.cat([k_prev, k], dim=2)\n",
                "        v_local = torch.cat([v_prev, v], dim=2)\n",
                "        \n",
                "        q = q.transpose(2, 3)\n",
                "        k_local = k_local.transpose(2, 3)\n",
                "        v_local = v_local.transpose(2, 3)\n",
                "        \n",
                "        attn = torch.matmul(q, k_local.transpose(-2, -1)) * self.scale\n",
                "        mask = torch.ones(block_size, 2*block_size, device=x.device).bool()\n",
                "        for i in range(block_size): mask[i, :block_size+i+1] = False\n",
                "        \n",
                "        attn = attn.masked_fill(mask, float('-inf'))\n",
                "        attn = F.softmax(attn, dim=-1)\n",
                "        attn = self.dropout(attn)\n",
                "        out = torch.matmul(attn, v_local)\n",
                "        out = out.transpose(2, 3).contiguous().view(batch_size, padded_len, self.d_model)\n",
                "        if pad_len > 0: out = out[:, :seq_len]\n",
                "        return self.out_proj(out)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.attn_norm = nn.LayerNorm(config.d_model)\n",
                "        self.ffn_norm = nn.LayerNorm(config.d_model)\n",
                "        self.attention = BlockLocalAttention(config) if config.use_block_local else MultiHeadAttention(config)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(config.d_model, config.d_ff, bias=False),\n",
                "            nn.GELU(),\n",
                "            nn.Dropout(config.dropout),\n",
                "            nn.Linear(config.d_ff, config.d_model, bias=False)\n",
                "        )\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        h = self.attn_norm(x)\n",
                "        x = x + self.dropout(self.attention(h))\n",
                "        h = self.ffn_norm(x)\n",
                "        x = x + self.dropout(self.ffn(h))\n",
                "        return x\n",
                "\n",
                "class SLM(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
                "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
                "        self.final_norm = nn.LayerNorm(config.d_model)\n",
                "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
                "        self.output.weight = self.token_emb.weight\n",
                "        self.apply(self._init_weights)\n",
                "\n",
                "    def _init_weights(self, module):\n",
                "        if isinstance(module, nn.Linear):\n",
                "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "        elif isinstance(module, nn.Embedding):\n",
                "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "\n",
                "    def forward(self, tokens):\n",
                "        x = self.token_emb(tokens)\n",
                "        for block in self.blocks: x = block(x)\n",
                "        x = self.final_norm(x)\n",
                "        return self.output(x)\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def generate(self, tokens, max_new_tokens=100, temperature=1.0):\n",
                "        self.eval()\n",
                "        for _ in range(max_new_tokens):\n",
                "            if tokens.size(1) >= self.config.max_seq_len: break\n",
                "            logits = self(tokens)[:, -1, :] / temperature\n",
                "            probs = F.softmax(logits, dim=-1)\n",
                "            next_token = torch.multinomial(probs, num_samples=1)\n",
                "            tokens = torch.cat([tokens, next_token], dim=1)\n",
                "        return tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 5. Training Pipeline\n",
                "\n",
                "Dataset streaming, curriculum learning setup, and training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextDataset(Dataset):\n",
                "    def __init__(self, file_paths: List[Path], tokenizer: Tokenizer, seq_len: int):\n",
                "        self.seq_len = seq_len\n",
                "        self.tokens = []\n",
                "        for path in file_paths:\n",
                "            if not path.exists(): continue\n",
                "            with open(path, 'r', encoding='utf-8') as f:\n",
                "                text = f.read()\n",
                "            self.tokens.append(torch.tensor(tokenizer.encode(text).ids, dtype=torch.long))\n",
                "        if self.tokens:\n",
                "            self.tokens = torch.cat(self.tokens)\n",
                "            self.n_chunks = (len(self.tokens) - 1) // seq_len\n",
                "        else:\n",
                "            self.n_chunks = 0\n",
                "\n",
                "    def __len__(self): return self.n_chunks\n",
                "    def __getitem__(self, idx):\n",
                "        start = idx * self.seq_len\n",
                "        end = start + self.seq_len + 1\n",
                "        chunk = self.tokens[start:end]\n",
                "        return chunk[:-1], chunk[1:]\n",
                "\n",
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    batch_size: int = 8\n",
                "    grad_accum: int = 4\n",
                "    lr: float = 3e-4\n",
                "    max_steps: int = 1000  # Reduced for demo purposes, original 50000\n",
                "    warmup_steps: int = 100\n",
                "    seq_len: int = 2048\n",
                "    \n",
                "def get_lr(step, config):\n",
                "    if step < config.warmup_steps:\n",
                "        return config.lr * step / config.warmup_steps\n",
                "    decay_ratio = (step - config.warmup_steps) / (config.max_steps - config.warmup_steps)\n",
                "    coeff = 0.5 * (1.0 + math.cos(math.pi * min(decay_ratio, 1.0)))\n",
                "    return config.lr * 0.1 + coeff * config.lr * 0.9"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_loop(model, dataset, config, phase_name):\n",
                "    loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=0.1)\n",
                "    model.train()\n",
                "    \n",
                "    step = 0\n",
                "    iter_loader = iter(loader)\n",
                "    print(f\"\\n=== Starting {phase_name} ==-\")\n",
                "    \n",
                "    while step < config.max_steps:\n",
                "        lr = get_lr(step, config)\n",
                "        for pg in optimizer.param_groups: pg['lr'] = lr\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss_accum = 0\n",
                "        for _ in range(config.grad_accum):\n",
                "            try:\n",
                "                x, y = next(iter_loader)\n",
                "            except StopIteration:\n",
                "                iter_loader = iter(loader)\n",
                "                x, y = next(iter_loader)\n",
                "            \n",
                "            x, y = x.to(device), y.to(device)\n",
                "            logits = model(x)\n",
                "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
                "            loss = loss / config.grad_accum\n",
                "            loss.backward()\n",
                "            loss_accum += loss.item()\n",
                "        \n",
                "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        step += 1\n",
                "        \n",
                "        if step % 50 == 0: # Log every 50 steps\n",
                "            print(f\"Step {step} | Loss: {loss_accum:.4f} | LR: {lr:.2e}\")\n",
                "            \n",
                "    # Save checkpoint\n",
                "    torch.save(model.state_dict(), CHECKPOINT_DIR / f\"{phase_name}_final.pt\")\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# 6. Execution & Evaluation\n",
                "\n",
                "Run the full pipeline (Demonstration Mode)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize Model for Phase A\n",
                "config = ModelConfig()\n",
                "model = SLM(config).to(device)\n",
                "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
                "\n",
                "# 2. Train Phase A (Short Demo)\n",
                "train_cfg = TrainingConfig(max_steps=50) # Very short for demo\n",
                "ds_a = TextDataset([DATA_DIR / 'base_stream.txt'], tokenizer, 2048)\n",
                "\n",
                "if len(ds_a) > 0:\n",
                "    model = train_loop(model, ds_a, train_cfg, \"PhaseA\")\n",
                "\n",
                "# 3. Evaluation\n",
                "eval_prompts = [\n",
                "    \"The stability of a positive feedback loop depends on\",\n",
                "    \"In a nuclear reactor, void coefficient refers to\"\n",
                "]\n",
                "\n",
                "print(\"\\n=== Evaluation ===\")\n",
                "model.eval()\n",
                "for p in eval_prompts:\n",
                "    tokens = torch.tensor([tokenizer.encode(p).ids], device=device)\n",
                "    out = model.generate(tokens, max_new_tokens=50)\n",
                "    print(f\"Prompt: {p}\")\n",
                "    print(f\"Generated: {tokenizer.decode(out[0].tolist())}\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}