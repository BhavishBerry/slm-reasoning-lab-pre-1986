{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ—ï¸ Model Architecture: Building the SLM from Scratch\n",
                "\n",
                "This notebook implements the complete model architecture for our 300M parameter SLM. We build everything from scratch:\n",
                "\n",
                "- **Rotary Position Embeddings (RoPE)** - for relative positional encoding\n",
                "- **Multi-Head Self-Attention** - the core of the transformer\n",
                "- **Block-Local Sparse Attention** - for efficient long-context handling\n",
                "- **Feed-Forward Networks** - the MLP layers\n",
                "\n",
                "**Target Config:**\n",
                "- Layers: 24\n",
                "- d_model: 1024\n",
                "- Heads: 16\n",
                "- FFN: 4096\n",
                "- ~300M parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, Tuple\n",
                "\n",
                "# Check if we have a GPU\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# For reproducibility\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ModelConfig:\n",
                "    \"\"\"All the knobs we can turn for our model.\"\"\"\n",
                "    \n",
                "    # Vocabulary (from our tokenizer)\n",
                "    vocab_size: int = 32000\n",
                "    \n",
                "    # Model dimensions\n",
                "    d_model: int = 1024        # Hidden size\n",
                "    n_layers: int = 24         # Number of transformer blocks\n",
                "    n_heads: int = 16          # Attention heads\n",
                "    d_ff: int = 4096           # Feed-forward intermediate size (4x d_model)\n",
                "    \n",
                "    # Context\n",
                "    max_seq_len: int = 2048    # Start with 2k, extend later\n",
                "    \n",
                "    # Block-local attention (for long context)\n",
                "    block_size: int = 512      # Each block attends locally\n",
                "    use_block_local: bool = False  # Off for Phase A, on for Phase B\n",
                "    \n",
                "    # Training\n",
                "    dropout: float = 0.1\n",
                "    \n",
                "    @property\n",
                "    def head_dim(self):\n",
                "        return self.d_model // self.n_heads\n",
                "\n",
                "# Create our config\n",
                "config = ModelConfig()\n",
                "print(f\"Head dimension: {config.head_dim}\")\n",
                "print(f\"Max sequence length: {config.max_seq_len}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Rotary Position Embeddings (RoPE)\n",
                "\n",
                "RoPE encodes position by **rotating** the query and key vectors. This has nice properties:\n",
                "- Position is encoded in the *relationship* between tokens, not absolute positions\n",
                "- Can extrapolate to longer sequences than trained on\n",
                "- No learned parameters - just math!\n",
                "\n",
                "The key idea: for position `m`, we rotate each pair of dimensions by `m * Î¸`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RotaryEmbedding(nn.Module):\n",
                "    \"\"\"\n",
                "    Rotary Position Embeddings.\n",
                "    \n",
                "    The magic: we precompute sin/cos values for all positions,\n",
                "    then apply them as rotations to Q and K vectors.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
                "        super().__init__()\n",
                "        self.dim = dim\n",
                "        self.max_seq_len = max_seq_len\n",
                "        self.base = base\n",
                "        \n",
                "        # Precompute the frequency bands\n",
                "        # Î¸_i = base^(-2i/d) for i = 0, 1, ..., d/2-1\n",
                "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
                "        self.register_buffer('inv_freq', inv_freq)\n",
                "        \n",
                "        # Precompute sin/cos for all positions\n",
                "        self._build_cache(max_seq_len)\n",
                "    \n",
                "    def _build_cache(self, seq_len: int):\n",
                "        \"\"\"Precompute sin and cos values.\"\"\"\n",
                "        positions = torch.arange(seq_len).float()\n",
                "        \n",
                "        # Outer product: [seq_len] x [dim/2] -> [seq_len, dim/2]\n",
                "        freqs = torch.outer(positions, self.inv_freq)\n",
                "        \n",
                "        # Stack to get [seq_len, dim] - each pair of dims shares a freq\n",
                "        emb = torch.cat([freqs, freqs], dim=-1)\n",
                "        \n",
                "        self.register_buffer('cos_cached', emb.cos())\n",
                "        self.register_buffer('sin_cached', emb.sin())\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"Return cos and sin for the given sequence length.\"\"\"\n",
                "        if seq_len > self.max_seq_len:\n",
                "            # Extend cache if needed (this is how we handle longer contexts)\n",
                "            self._build_cache(seq_len)\n",
                "        \n",
                "        return (\n",
                "            self.cos_cached[:seq_len].to(x.device),\n",
                "            self.sin_cached[:seq_len].to(x.device)\n",
                "        )\n",
                "\n",
                "\n",
                "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
                "    \"\"\"Rotate half the hidden dims of x.\"\"\"\n",
                "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
                "    return torch.cat([-x2, x1], dim=-1)\n",
                "\n",
                "\n",
                "def apply_rotary_emb(q: torch.Tensor, k: torch.Tensor, \n",
                "                     cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Apply rotary embeddings to query and key tensors.\n",
                "    \n",
                "    This is the actual rotation: x * cos + rotate_half(x) * sin\n",
                "    \"\"\"\n",
                "    # Reshape for broadcasting: [seq_len, dim] -> [1, seq_len, 1, dim]\n",
                "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
                "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
                "    \n",
                "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
                "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
                "    \n",
                "    return q_embed, k_embed\n",
                "\n",
                "# Quick test\n",
                "rope = RotaryEmbedding(dim=64, max_seq_len=2048)\n",
                "cos, sin = rope(torch.randn(1), seq_len=100)\n",
                "print(f\"RoPE cache shape: cos={cos.shape}, sin={sin.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Multi-Head Self-Attention\n",
                "\n",
                "The core of the transformer. Each head learns different attention patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    \"\"\"\n",
                "    Multi-head self-attention with RoPE.\n",
                "    \n",
                "    For a decoder model, we use causal masking - each token can only\n",
                "    attend to previous tokens (and itself).\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.n_heads = config.n_heads\n",
                "        self.head_dim = config.head_dim\n",
                "        self.d_model = config.d_model\n",
                "        \n",
                "        # Q, K, V projections (could be fused, but this is clearer)\n",
                "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        \n",
                "        # RoPE for this layer\n",
                "        self.rotary = RotaryEmbedding(self.head_dim, config.max_seq_len)\n",
                "        \n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "        \n",
                "        # Scaling factor for attention scores\n",
                "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        \n",
                "        # Project to Q, K, V\n",
                "        q = self.q_proj(x)\n",
                "        k = self.k_proj(x)\n",
                "        v = self.v_proj(x)\n",
                "        \n",
                "        # Reshape: [batch, seq, d_model] -> [batch, seq, n_heads, head_dim]\n",
                "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
                "        \n",
                "        # Apply RoPE to Q and K (not V!)\n",
                "        cos, sin = self.rotary(q, seq_len)\n",
                "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
                "        \n",
                "        # Transpose for attention: [batch, n_heads, seq, head_dim]\n",
                "        q = q.transpose(1, 2)\n",
                "        k = k.transpose(1, 2)\n",
                "        v = v.transpose(1, 2)\n",
                "        \n",
                "        # Attention scores: [batch, n_heads, seq, seq]\n",
                "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        # Causal mask - each position can only see previous positions\n",
                "        if mask is None:\n",
                "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
                "        attn = attn.masked_fill(mask, float('-inf'))\n",
                "        \n",
                "        # Softmax and dropout\n",
                "        attn = F.softmax(attn, dim=-1)\n",
                "        attn = self.dropout(attn)\n",
                "        \n",
                "        # Apply attention to values\n",
                "        out = torch.matmul(attn, v)  # [batch, n_heads, seq, head_dim]\n",
                "        \n",
                "        # Reshape back: [batch, seq, d_model]\n",
                "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
                "        \n",
                "        return self.out_proj(out)\n",
                "\n",
                "# Test it\n",
                "attn = MultiHeadAttention(config)\n",
                "x = torch.randn(2, 128, config.d_model)  # [batch=2, seq=128, d_model]\n",
                "out = attn(x)\n",
                "print(f\"Attention output shape: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Block-Local Sparse Attention\n",
                "\n",
                "For long contexts (4k-5k tokens), full attention becomes expensive: O(NÂ²).\n",
                "\n",
                "**Block-local attention** splits the sequence into blocks. Each token attends to:\n",
                "- Its own block (full attention within)\n",
                "- The previous block (for continuity)\n",
                "\n",
                "This gives us O(N Ã— block_size) instead of O(NÂ²)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BlockLocalAttention(nn.Module):\n",
                "    \"\"\"\n",
                "    Block-local sparse attention for efficient long-context modeling.\n",
                "    \n",
                "    Each block of 'block_size' tokens attends to itself and the previous block.\n",
                "    This is much more efficient for long sequences.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.n_heads = config.n_heads\n",
                "        self.head_dim = config.head_dim\n",
                "        self.d_model = config.d_model\n",
                "        self.block_size = config.block_size\n",
                "        \n",
                "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
                "        \n",
                "        self.rotary = RotaryEmbedding(self.head_dim, config.max_seq_len)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        block_size = self.block_size\n",
                "        \n",
                "        # Pad sequence to be divisible by block_size\n",
                "        pad_len = (block_size - seq_len % block_size) % block_size\n",
                "        if pad_len > 0:\n",
                "            x = F.pad(x, (0, 0, 0, pad_len))\n",
                "        \n",
                "        padded_len = x.shape[1]\n",
                "        n_blocks = padded_len // block_size\n",
                "        \n",
                "        # Project Q, K, V\n",
                "        q = self.q_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        k = self.k_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        v = self.v_proj(x).view(batch_size, padded_len, self.n_heads, self.head_dim)\n",
                "        \n",
                "        # Apply RoPE\n",
                "        cos, sin = self.rotary(q, padded_len)\n",
                "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
                "        \n",
                "        # Reshape into blocks: [batch, n_blocks, block_size, n_heads, head_dim]\n",
                "        q = q.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        k = k.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        v = v.view(batch_size, n_blocks, block_size, self.n_heads, self.head_dim)\n",
                "        \n",
                "        # For each block, we attend to current + previous block\n",
                "        # Shift K and V to create \"previous block\" context\n",
                "        k_prev = F.pad(k, (0, 0, 0, 0, 0, 0, 1, 0))[:, :-1]  # Shift right, pad start\n",
                "        v_prev = F.pad(v, (0, 0, 0, 0, 0, 0, 1, 0))[:, :-1]\n",
                "        \n",
                "        # Concatenate current and previous block keys/values\n",
                "        # [batch, n_blocks, 2*block_size, n_heads, head_dim]\n",
                "        k_local = torch.cat([k_prev, k], dim=2)\n",
                "        v_local = torch.cat([v_prev, v], dim=2)\n",
                "        \n",
                "        # Transpose for attention: [batch, n_blocks, n_heads, block_size, 2*block_size]\n",
                "        q = q.transpose(2, 3)\n",
                "        k_local = k_local.transpose(2, 3)\n",
                "        v_local = v_local.transpose(2, 3)\n",
                "        \n",
                "        # Compute attention\n",
                "        attn = torch.matmul(q, k_local.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        # Causal mask within the local window\n",
                "        # Query position i in block can attend to key positions 0..(block_size + i)\n",
                "        mask = torch.ones(block_size, 2 * block_size, device=x.device).bool()\n",
                "        for i in range(block_size):\n",
                "            # Can attend to previous block + positions 0..i in current block\n",
                "            mask[i, :block_size + i + 1] = False\n",
                "        \n",
                "        attn = attn.masked_fill(mask, float('-inf'))\n",
                "        attn = F.softmax(attn, dim=-1)\n",
                "        attn = self.dropout(attn)\n",
                "        \n",
                "        # Apply attention\n",
                "        out = torch.matmul(attn, v_local)\n",
                "        \n",
                "        # Reshape back\n",
                "        out = out.transpose(2, 3).contiguous()\n",
                "        out = out.view(batch_size, padded_len, self.d_model)\n",
                "        \n",
                "        # Remove padding\n",
                "        if pad_len > 0:\n",
                "            out = out[:, :seq_len]\n",
                "        \n",
                "        return self.out_proj(out)\n",
                "\n",
                "# Test it\n",
                "block_attn = BlockLocalAttention(config)\n",
                "x = torch.randn(2, 1024, config.d_model)\n",
                "out = block_attn(x)\n",
                "print(f\"Block-local attention output shape: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Feed-Forward Network\n",
                "\n",
                "The other half of each transformer block - a simple 2-layer MLP with GeLU activation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForward(nn.Module):\n",
                "    \"\"\"\n",
                "    Position-wise feed-forward network.\n",
                "    \n",
                "    Two linear layers with GELU activation in between.\n",
                "    Expands from d_model to d_ff (4x), then back down.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n",
                "        self.fc2 = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        # Expand -> activate -> contract\n",
                "        x = self.fc1(x)\n",
                "        x = F.gelu(x)  # Smoother than ReLU\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Test\n",
                "ffn = FeedForward(config)\n",
                "out = ffn(torch.randn(2, 128, config.d_model))\n",
                "print(f\"FFN output shape: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Transformer Block\n",
                "\n",
                "Now we combine attention + feed-forward with residual connections and layer norm."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    \"\"\"\n",
                "    A single transformer block: attention + FFN with residuals.\n",
                "    \n",
                "    We use pre-norm (LayerNorm before each sublayer) - this tends to\n",
                "    train more stably than post-norm.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Pre-norm layers\n",
                "        self.attn_norm = nn.LayerNorm(config.d_model)\n",
                "        self.ffn_norm = nn.LayerNorm(config.d_model)\n",
                "        \n",
                "        # Choose attention type based on config\n",
                "        if config.use_block_local:\n",
                "            self.attention = BlockLocalAttention(config)\n",
                "        else:\n",
                "            self.attention = MultiHeadAttention(config)\n",
                "        \n",
                "        self.ffn = FeedForward(config)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        # Attention with residual\n",
                "        h = self.attn_norm(x)\n",
                "        h = self.attention(h)\n",
                "        x = x + self.dropout(h)\n",
                "        \n",
                "        # FFN with residual\n",
                "        h = self.ffn_norm(x)\n",
                "        h = self.ffn(h)\n",
                "        x = x + self.dropout(h)\n",
                "        \n",
                "        return x\n",
                "\n",
                "# Test\n",
                "block = TransformerBlock(config)\n",
                "out = block(torch.randn(2, 128, config.d_model))\n",
                "print(f\"Transformer block output shape: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Complete Model\n",
                "\n",
                "The full decoder model: embeddings -> N transformer blocks -> output projection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SLM(nn.Module):\n",
                "    \"\"\"\n",
                "    The complete Small Language Model.\n",
                "    \n",
                "    Decoder-only transformer for autoregressive language modeling.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        \n",
                "        # Token embeddings\n",
                "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
                "        \n",
                "        # Stack of transformer blocks\n",
                "        self.blocks = nn.ModuleList([\n",
                "            TransformerBlock(config) for _ in range(config.n_layers)\n",
                "        ])\n",
                "        \n",
                "        # Final layer norm (before output projection)\n",
                "        self.final_norm = nn.LayerNorm(config.d_model)\n",
                "        \n",
                "        # Output projection (ties with token embeddings for efficiency)\n",
                "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
                "        \n",
                "        # Weight tying - embedding and output share weights\n",
                "        self.output.weight = self.token_emb.weight\n",
                "        \n",
                "        # Initialize weights\n",
                "        self._init_weights()\n",
                "    \n",
                "    def _init_weights(self):\n",
                "        \"\"\"Initialize weights with small values for stable training.\"\"\"\n",
                "        for p in self.parameters():\n",
                "            if p.dim() > 1:\n",
                "                nn.init.normal_(p, mean=0.0, std=0.02)\n",
                "    \n",
                "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Args:\n",
                "            tokens: [batch, seq_len] token IDs\n",
                "        \n",
                "        Returns:\n",
                "            logits: [batch, seq_len, vocab_size] predictions for each position\n",
                "        \"\"\"\n",
                "        # Embed tokens\n",
                "        x = self.token_emb(tokens)  # [batch, seq, d_model]\n",
                "        \n",
                "        # Pass through transformer blocks\n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "        \n",
                "        # Final norm and project to vocabulary\n",
                "        x = self.final_norm(x)\n",
                "        logits = self.output(x)\n",
                "        \n",
                "        return logits\n",
                "    \n",
                "    def generate(self, prompt_tokens: torch.Tensor, max_new_tokens: int = 100,\n",
                "                 temperature: float = 1.0) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Generate text autoregressively.\n",
                "        \n",
                "        Simple greedy/temperature sampling - nothing fancy.\n",
                "        \"\"\"\n",
                "        self.eval()\n",
                "        tokens = prompt_tokens.clone()\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for _ in range(max_new_tokens):\n",
                "                # Get predictions for last position\n",
                "                logits = self(tokens)[:, -1, :]  # [batch, vocab]\n",
                "                \n",
                "                # Apply temperature\n",
                "                logits = logits / temperature\n",
                "                \n",
                "                # Sample next token\n",
                "                probs = F.softmax(logits, dim=-1)\n",
                "                next_token = torch.multinomial(probs, num_samples=1)\n",
                "                \n",
                "                # Append to sequence\n",
                "                tokens = torch.cat([tokens, next_token], dim=1)\n",
                "                \n",
                "                # Stop at max length\n",
                "                if tokens.shape[1] >= self.config.max_seq_len:\n",
                "                    break\n",
                "        \n",
                "        return tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Parameter Count\n",
                "\n",
                "Let's verify we're in the ~300M range."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_parameters(model):\n",
                "    \"\"\"Count trainable parameters.\"\"\"\n",
                "    total = sum(p.numel() for p in model.parameters())\n",
                "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    return total, trainable\n",
                "\n",
                "# Create the model\n",
                "model = SLM(config)\n",
                "\n",
                "total, trainable = count_parameters(model)\n",
                "print(f\"Total parameters: {total:,}\")\n",
                "print(f\"Trainable parameters: {trainable:,}\")\n",
                "print(f\"\\nThat's approximately {total / 1e6:.0f}M parameters\")\n",
                "\n",
                "# Breakdown by component\n",
                "print(\"\\nParameter breakdown:\")\n",
                "print(f\"  Token embeddings: {model.token_emb.weight.numel():,}\")\n",
                "print(f\"  Transformer blocks: {sum(p.numel() for p in model.blocks.parameters()):,}\")\n",
                "print(f\"  Final norm: {sum(p.numel() for p in model.final_norm.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick forward pass test\n",
                "batch_size = 2\n",
                "seq_len = 256\n",
                "\n",
                "# Random token IDs\n",
                "dummy_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
                "\n",
                "# Forward pass\n",
                "logits = model(dummy_tokens)\n",
                "print(f\"Input shape: {dummy_tokens.shape}\")\n",
                "print(f\"Output shape: {logits.shape}\")\n",
                "print(f\"\\nâœ“ Model works!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "We've built:\n",
                "\n",
                "1. **RoPE** - Rotary position embeddings for relative position encoding\n",
                "2. **Multi-Head Attention** - Standard causal self-attention with RoPE\n",
                "3. **Block-Local Attention** - Efficient sparse attention for long contexts\n",
                "4. **Transformer Block** - Attention + FFN with pre-norm and residuals\n",
                "5. **Complete SLM** - ~300M parameter decoder model\n",
                "\n",
                "**Next:** In notebook 04, we'll set up the training pipeline to actually train this model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save config for later use\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "config_path = Path(\"../configs\")\n",
                "config_path.mkdir(exist_ok=True)\n",
                "\n",
                "# Save as JSON\n",
                "config_dict = {\n",
                "    'vocab_size': config.vocab_size,\n",
                "    'd_model': config.d_model,\n",
                "    'n_layers': config.n_layers,\n",
                "    'n_heads': config.n_heads,\n",
                "    'd_ff': config.d_ff,\n",
                "    'max_seq_len': config.max_seq_len,\n",
                "    'block_size': config.block_size,\n",
                "    'dropout': config.dropout\n",
                "}\n",
                "\n",
                "with open(config_path / 'model_config.json', 'w') as f:\n",
                "    json.dump(config_dict, f, indent=2)\n",
                "\n",
                "print(f\"âœ“ Config saved to {config_path / 'model_config.json'}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}