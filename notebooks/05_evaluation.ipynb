{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¬ Evaluation: Zero-Shot Reasoning Assessment\n",
                "\n",
                "This notebook evaluates our trained SLM using **zero-shot analytical prompts**. No examples, no formatting instructions, no chain-of-thought forcing.\n",
                "\n",
                "**Evaluation Philosophy:**\n",
                "- Open-ended prompts about system behavior\n",
                "- Focus on reasoning quality, not benchmark scores\n",
                "- Both successes and failures are meaningful data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup & Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from tokenizers import Tokenizer\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "torch.manual_seed(42)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TOKENIZER_PATH = Path(\"../tokenizer/tokenizer.json\")\n",
                "CHECKPOINT_DIR = Path(\"../checkpoints\")\n",
                "\n",
                "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
                "print(f\"âœ“ Tokenizer loaded: {tokenizer.get_vocab_size()} tokens\")\n",
                "\n",
                "# List available checkpoints\n",
                "if CHECKPOINT_DIR.exists():\n",
                "    checkpoints = list(CHECKPOINT_DIR.glob(\"*.pt\"))\n",
                "    print(f\"\\nAvailable checkpoints:\")\n",
                "    for cp in sorted(checkpoints):\n",
                "        print(f\"  - {cp.name}\")\n",
                "else:\n",
                "    print(\"No checkpoints found - run training first!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model architecture (copy from notebook 03 or import from saved module)\n",
                "# For evaluation, we need the same model definition used during training\n",
                "\n",
                "def load_model_from_checkpoint(checkpoint_path, model_class, config):\n",
                "    \"\"\"Load a trained model from checkpoint.\"\"\"\n",
                "    model = model_class(config)\n",
                "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
                "    model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    model.to(device)\n",
                "    model.eval()\n",
                "    print(f\"âœ“ Loaded checkpoint: {checkpoint_path.name}\")\n",
                "    print(f\"  Step: {checkpoint.get('step', 'unknown')}\")\n",
                "    print(f\"  Loss: {checkpoint.get('loss', 'unknown'):.4f}\")\n",
                "    return model\n",
                "\n",
                "# NOTE: Uncomment and modify when you have a trained model\n",
                "# model = load_model_from_checkpoint(\n",
                "#     CHECKPOINT_DIR / \"phase_c_final_step5000.pt\",\n",
                "#     SLM,\n",
                "#     ModelConfig(use_block_local=True, max_seq_len=4096)\n",
                "# )\n",
                "\n",
                "print(\"Model loading function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Text Generation Utilities\n",
                "\n",
                "Simple generation with temperature control. No beam search, no fancy sampling - just the basics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(model, prompt, max_tokens=200, temperature=0.8):\n",
                "    \"\"\"\n",
                "    Generate text from a prompt.\n",
                "    \n",
                "    Lower temperature = more focused/deterministic\n",
                "    Higher temperature = more creative/diverse\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Tokenize prompt\n",
                "    encoding = tokenizer.encode(prompt)\n",
                "    tokens = torch.tensor([encoding.ids], dtype=torch.long, device=device)\n",
                "    \n",
                "    # Get special token IDs\n",
                "    eos_id = tokenizer.token_to_id(\"<EOS>\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for _ in range(max_tokens):\n",
                "            # Get logits for last position\n",
                "            logits = model(tokens)[:, -1, :]\n",
                "            \n",
                "            # Apply temperature\n",
                "            logits = logits / temperature\n",
                "            \n",
                "            # Sample next token\n",
                "            probs = F.softmax(logits, dim=-1)\n",
                "            next_token = torch.multinomial(probs, num_samples=1)\n",
                "            \n",
                "            # Stop at EOS\n",
                "            if next_token.item() == eos_id:\n",
                "                break\n",
                "            \n",
                "            tokens = torch.cat([tokens, next_token], dim=1)\n",
                "    \n",
                "    # Decode output\n",
                "    output_ids = tokens[0].tolist()\n",
                "    output_text = tokenizer.decode(output_ids)\n",
                "    \n",
                "    return output_text\n",
                "\n",
                "print(\"Generation function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Evaluation Prompts\n",
                "\n",
                "Our prompts are designed to test **reasoning about system behavior** without asking for specific conclusions. The model should:\n",
                "- Identify causal dependencies\n",
                "- State assumptions explicitly\n",
                "- Handle uncertainty appropriately"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation prompts - designed to test reasoning, not recall\n",
                "\n",
                "EVAL_PROMPTS = {\n",
                "    \"feedback_loop\": \"\"\"\n",
                "Consider a system where the output of a process influences its own input through \n",
                "a delay mechanism. As the delay increases, describe what happens to the system's \n",
                "stability and explain the underlying principles.\n",
                "\"\"\".strip(),\n",
                "    \n",
                "    \"thermal_coupling\": \"\"\"\n",
                "A reactor vessel contains a fluid whose temperature affects its own heat transfer \n",
                "properties. As temperature rises, the fluid becomes less effective at removing heat. \n",
                "Analyze the implications for system behavior.\n",
                "\"\"\".strip(),\n",
                "    \n",
                "    \"control_failure\": \"\"\"\n",
                "An automated control system is designed to maintain a variable at a setpoint. \n",
                "The sensor measuring this variable has a measurement lag. Under what conditions \n",
                "might this configuration lead to oscillations or instability?\n",
                "\"\"\".strip(),\n",
                "    \n",
                "    \"redundancy_analysis\": \"\"\"\n",
                "A safety system consists of three independent channels, any one of which can \n",
                "trigger a protective action. Discuss the factors that would determine whether \n",
                "this redundancy provides adequate protection.\n",
                "\"\"\".strip(),\n",
                "    \n",
                "    \"transient_behavior\": \"\"\"\n",
                "A physical system is operating at steady state when a step change is applied \n",
                "to one of its inputs. The system response initially moves in one direction, \n",
                "then reverses. What does this inverse response indicate about the system's \n",
                "internal structure?\n",
                "\"\"\".strip(),\n",
                "}\n",
                "\n",
                "print(f\"Prepared {len(EVAL_PROMPTS)} evaluation prompts:\")\n",
                "for name in EVAL_PROMPTS:\n",
                "    print(f\"  - {name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Running Evaluation\n",
                "\n",
                "Generate responses and save for analysis. We'll compare outputs at different temperatures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_evaluation(model, prompts, temperatures=[0.7, 1.0]):\n",
                "    \"\"\"\n",
                "    Run all evaluation prompts at different temperatures.\n",
                "    \n",
                "    Returns a dict of results for analysis.\n",
                "    \"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    for name, prompt in prompts.items():\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"Prompt: {name}\")\n",
                "        print(f\"{'='*60}\")\n",
                "        print(prompt)\n",
                "        \n",
                "        results[name] = {\"prompt\": prompt, \"responses\": {}}\n",
                "        \n",
                "        for temp in temperatures:\n",
                "            print(f\"\\n--- Temperature {temp} ---\")\n",
                "            response = generate_text(model, prompt, max_tokens=300, temperature=temp)\n",
                "            results[name][\"responses\"][temp] = response\n",
                "            print(response)\n",
                "    \n",
                "    return results\n",
                "\n",
                "# NOTE: Uncomment when you have a loaded model\n",
                "# results = run_evaluation(model, EVAL_PROMPTS)\n",
                "\n",
                "print(\"Evaluation function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Qualitative Assessment Criteria\n",
                "\n",
                "We evaluate responses on these dimensions (manually, not automatically):\n",
                "\n",
                "| Criterion | Description |\n",
                "|-----------|-------------|\n",
                "| **Causal Depth** | Does the response identify cause-effect relationships? |\n",
                "| **Conditional Reasoning** | Does it use \"if-then\" logic appropriately? |\n",
                "| **Explicit Uncertainty** | Does it acknowledge what it doesn't know? |\n",
                "| **Consistency** | Are claims internally consistent? |\n",
                "| **Assumption Clarity** | Are underlying assumptions stated? |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def assess_response(response):\n",
                "    \"\"\"\n",
                "    Manual assessment template - prints prompts for human evaluation.\n",
                "    \"\"\"\n",
                "    print(\"Response to assess:\")\n",
                "    print(\"-\" * 40)\n",
                "    print(response)\n",
                "    print(\"-\" * 40)\n",
                "    print(\"\\nAssessment criteria (rate 1-5):\")\n",
                "    print(\"  1. Causal Depth: Does it identify cause-effect relationships?\")\n",
                "    print(\"  2. Conditional Reasoning: Does it use if-then logic?\")\n",
                "    print(\"  3. Explicit Uncertainty: Does it acknowledge unknowns?\")\n",
                "    print(\"  4. Consistency: Are claims internally consistent?\")\n",
                "    print(\"  5. Assumption Clarity: Are assumptions stated?\")\n",
                "    print(\"\\nNotes:\")\n",
                "    print(\"  - What reasoning patterns are present?\")\n",
                "    print(\"  - What's missing or incorrect?\")\n",
                "    print(\"  - Would this be useful to a human analyst?\")\n",
                "\n",
                "# Example usage (with dummy response)\n",
                "assess_response(\"[Model response would appear here]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Long-Context Evaluation\n",
                "\n",
                "Test whether the model maintains coherence over extended contexts. We provide a long system description and check if conclusions remain consistent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LONG_CONTEXT_PROMPT = \"\"\"\n",
                "The following describes a complex industrial control system:\n",
                "\n",
                "A chemical reactor operates with three primary control loops. The first loop \n",
                "regulates temperature by adjusting coolant flow rate. The temperature sensor \n",
                "has a 30-second measurement delay. The second loop controls pressure by \n",
                "modulating a relief valve. Pressure and temperature are coupled - rising \n",
                "temperature increases pressure according to the ideal gas law. The third \n",
                "loop maintains reactant concentration by controlling feed rate.\n",
                "\n",
                "The system has the following characteristics:\n",
                "- Temperature setpoint: 350Â°C\n",
                "- Pressure operating range: 10-15 bar\n",
                "- Coolant flow capacity: 0-100 L/min\n",
                "- Feed rate range: 0-50 kg/hr\n",
                "\n",
                "An operator initiates a production rate increase by raising the feed rate \n",
                "setpoint from 30 to 45 kg/hr. The reaction is exothermic.\n",
                "\n",
                "Analyze the sequence of events that would follow this change, considering \n",
                "the interactions between all three control loops. Identify any potential \n",
                "stability concerns.\n",
                "\"\"\".strip()\n",
                "\n",
                "print(\"Long-context prompt ready\")\n",
                "print(f\"Prompt length: {len(tokenizer.encode(LONG_CONTEXT_PROMPT).ids)} tokens\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_long_context(model, prompt, n_runs=3):\n",
                "    \"\"\"\n",
                "    Run the same long-context prompt multiple times.\n",
                "    \n",
                "    Check for:\n",
                "    - Consistency of conclusions across runs\n",
                "    - Reference to earlier parts of the prompt\n",
                "    - Absence of contradictions\n",
                "    \"\"\"\n",
                "    responses = []\n",
                "    \n",
                "    for i in range(n_runs):\n",
                "        print(f\"\\nRun {i+1}/{n_runs}\")\n",
                "        print(\"-\" * 40)\n",
                "        response = generate_text(model, prompt, max_tokens=400, temperature=0.7)\n",
                "        responses.append(response)\n",
                "        print(response[:500] + \"...\" if len(response) > 500 else response)\n",
                "    \n",
                "    return responses\n",
                "\n",
                "# NOTE: Uncomment when model is loaded\n",
                "# long_context_results = evaluate_long_context(model, LONG_CONTEXT_PROMPT)\n",
                "\n",
                "print(\"Long-context evaluation ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Comparative Analysis: 2k vs 5k Context\n",
                "\n",
                "Compare model behavior at different context lengths to verify the context extension worked."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_context_lengths(model_2k, model_5k, prompt):\n",
                "    \"\"\"\n",
                "    Compare outputs from models trained at different context lengths.\n",
                "    \n",
                "    We expect the 5k model to:\n",
                "    - Reference more of the prompt context\n",
                "    - Maintain coherence over longer outputs\n",
                "    - Show similar reasoning patterns on short prompts\n",
                "    \"\"\"\n",
                "    print(\"2k Context Model:\")\n",
                "    print(\"-\" * 40)\n",
                "    response_2k = generate_text(model_2k, prompt, max_tokens=300)\n",
                "    print(response_2k)\n",
                "    \n",
                "    print(\"\\n5k Context Model:\")\n",
                "    print(\"-\" * 40)\n",
                "    response_5k = generate_text(model_5k, prompt, max_tokens=300)\n",
                "    print(response_5k)\n",
                "    \n",
                "    return response_2k, response_5k\n",
                "\n",
                "# This requires loading both Phase A and Phase C checkpoints\n",
                "print(\"Context comparison function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Perplexity Calculation\n",
                "\n",
                "While we prioritize qualitative assessment, perplexity gives us a rough quantitative measure of model fit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_perplexity(model, text, max_length=2048):\n",
                "    \"\"\"\n",
                "    Calculate perplexity on a text sample.\n",
                "    \n",
                "    Lower perplexity = model assigns higher probability to the text.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    encoding = tokenizer.encode(text)\n",
                "    tokens = torch.tensor([encoding.ids[:max_length]], dtype=torch.long, device=device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        logits = model(tokens)\n",
                "        \n",
                "        # Shift for next-token prediction\n",
                "        shift_logits = logits[:, :-1, :].contiguous()\n",
                "        shift_labels = tokens[:, 1:].contiguous()\n",
                "        \n",
                "        # Cross-entropy loss\n",
                "        loss = F.cross_entropy(\n",
                "            shift_logits.view(-1, shift_logits.size(-1)),\n",
                "            shift_labels.view(-1)\n",
                "        )\n",
                "        \n",
                "        perplexity = torch.exp(loss).item()\n",
                "    \n",
                "    return perplexity\n",
                "\n",
                "print(\"Perplexity calculation ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perplexity on different text types\n",
                "# This helps us understand what the model \"knows\" well\n",
                "\n",
                "TEST_TEXTS = {\n",
                "    \"technical\": \"The transfer function of a closed-loop control system determines its stability characteristics.\",\n",
                "    \"narrative\": \"The engineer walked into the control room and noticed the temperature gauge rising.\",\n",
                "    \"mathematical\": \"Given that f(x) = x^2 + 2x + 1, the derivative f'(x) = 2x + 2.\",\n",
                "}\n",
                "\n",
                "# NOTE: Uncomment when model is loaded\n",
                "# print(\"Perplexity by text type:\")\n",
                "# for name, text in TEST_TEXTS.items():\n",
                "#     ppl = calculate_perplexity(model, text)\n",
                "#     print(f\"  {name}: {ppl:.2f}\")\n",
                "\n",
                "print(\"Test texts ready for perplexity evaluation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Saving Evaluation Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "\n",
                "def save_evaluation_results(results, output_dir=\"../eval_results\"):\n",
                "    \"\"\"Save evaluation results to JSON for later analysis.\"\"\"\n",
                "    output_path = Path(output_dir)\n",
                "    output_path.mkdir(exist_ok=True)\n",
                "    \n",
                "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    filename = output_path / f\"eval_{timestamp}.json\"\n",
                "    \n",
                "    with open(filename, 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    \n",
                "    print(f\"âœ“ Results saved to: {filename}\")\n",
                "    return filename\n",
                "\n",
                "print(\"Results saving function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook provides the evaluation framework for our SLM:\n",
                "\n",
                "1. **Zero-shot prompts** - Testing reasoning without examples\n",
                "2. **Qualitative criteria** - Causal depth, consistency, uncertainty handling\n",
                "3. **Long-context tests** - Verify extended context capability\n",
                "4. **Perplexity analysis** - Quantitative baseline measure\n",
                "\n",
                "**Key Principle:** Both successes and failures are valuable. The goal is insight into reasoning emergence, not a leaderboard score.\n",
                "\n",
                "**To use this notebook:**\n",
                "1. Run training (notebook 04) to generate checkpoints\n",
                "2. Load the Phase C checkpoint\n",
                "3. Run evaluation prompts\n",
                "4. Manually assess responses using the criteria"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}