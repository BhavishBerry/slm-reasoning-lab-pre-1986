{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¤ Tokenizer Training: Building a BPE Tokenizer\n",
                "\n",
                "Before we can train our SLM, we need to convert text into tokens. This notebook trains a **Byte Pair Encoding (BPE)** tokenizer on our pre-1986 corpus.\n",
                "\n",
                "**Why tokenization matters:** A poorly designed tokenizer fragments technical terms and equations, making it harder for the model to learn. We want tokens that respect word boundaries and technical vocabulary."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup\n",
                "\n",
                "We'll use the Hugging Face `tokenizers` library - it's fast and flexible."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You might need: pip install tokenizers\n",
                "from tokenizers import Tokenizer\n",
                "from tokenizers.models import BPE\n",
                "from tokenizers.trainers import BpeTrainer\n",
                "from tokenizers.pre_tokenizers import Whitespace\n",
                "from tokenizers.processors import TemplateProcessing\n",
                "from pathlib import Path\n",
                "\n",
                "DATA_DIR = Path(\"../data/pre1986_training_streams_v1_FINAL\")\n",
                "TOKENIZER_DIR = Path(\"../tokenizer\")\n",
                "TOKENIZER_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "print(\"âœ“ Imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. How BPE Works (Quick Overview)\n",
                "\n",
                "**Byte Pair Encoding** builds a vocabulary by:\n",
                "1. Starting with individual characters\n",
                "2. Finding the most frequent pair of adjacent tokens\n",
                "3. Merging that pair into a new token\n",
                "4. Repeating until we hit our target vocabulary size\n",
                "\n",
                "For example:\n",
                "```\n",
                "\"low\" \"lower\" \"newest\" \"widest\"\n",
                "â†’ After BPE: \"low\" \"low\" \"er\" \"new\" \"est\" \"wid\" \"est\"\n",
                "â†’ Common subwords like \"est\" become their own tokens\n",
                "```\n",
                "\n",
                "This is great for handling rare technical terms - even if we've never seen \"thermocouple\" before, we might have tokens for \"thermo\" and \"couple\"."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Configure the Tokenizer\n",
                "\n",
                "Key decisions:\n",
                "- **Vocab size: 32,000** - Large enough for technical vocabulary, small enough to keep embeddings manageable\n",
                "- **Special tokens:** We need `<EOS>`, `<PAD>`, and `<UNK>` at minimum"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize a blank BPE tokenizer\n",
                "tokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
                "\n",
                "# Pre-tokenization: split on whitespace first\n",
                "# This prevents weird merges across word boundaries\n",
                "tokenizer.pre_tokenizer = Whitespace()\n",
                "\n",
                "# Configure the trainer\n",
                "trainer = BpeTrainer(\n",
                "    vocab_size=32000,\n",
                "    min_frequency=2,  # Token must appear at least twice\n",
                "    special_tokens=[\n",
                "        \"<PAD>\",   # Padding for batching\n",
                "        \"<UNK>\",   # Unknown tokens\n",
                "        \"<EOS>\",   # End of sequence (document boundary)\n",
                "        \"<BOS>\",   # Beginning of sequence (optional, but nice to have)\n",
                "    ],\n",
                "    show_progress=True\n",
                ")\n",
                "\n",
                "print(\"Tokenizer configured with:\")\n",
                "print(f\"  - Target vocab size: 32,000\")\n",
                "print(f\"  - Special tokens: <PAD>, <UNK>, <EOS>, <BOS>\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Train the Tokenizer\n",
                "\n",
                "**Important:** We train ONLY on the base pretraining corpus, not the fine-tuning data.\n",
                "\n",
                "Why? The tokenizer should represent general language patterns. Fine-tuning data is for shaping *reasoning*, not vocabulary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training on base_stream only\n",
                "training_files = [str(DATA_DIR / \"base_stream.txt\")]\n",
                "\n",
                "print(f\"Training tokenizer on: {training_files}\")\n",
                "print(\"This might take a minute...\\n\")\n",
                "\n",
                "# Train!\n",
                "tokenizer.train(training_files, trainer)\n",
                "\n",
                "print(f\"\\nâœ“ Training complete!\")\n",
                "print(f\"  Final vocabulary size: {tokenizer.get_vocab_size():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Test the Tokenizer\n",
                "\n",
                "Let's see how it handles different types of text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_tokenization(text):\n",
                "    \"\"\"Display how text gets tokenized.\"\"\"\n",
                "    encoding = tokenizer.encode(text)\n",
                "    print(f\"Input:  '{text}'\")\n",
                "    print(f\"Tokens: {encoding.tokens}\")\n",
                "    print(f\"IDs:    {encoding.ids}\")\n",
                "    print(f\"Count:  {len(encoding.tokens)} tokens\")\n",
                "    print()\n",
                "\n",
                "# Test on various inputs\n",
                "print(\"=== Regular English ===\")\n",
                "show_tokenization(\"The quick brown fox jumps over the lazy dog.\")\n",
                "\n",
                "print(\"=== Technical Text ===\")\n",
                "show_tokenization(\"The thermocouple measures temperature differentials.\")\n",
                "\n",
                "print(\"=== Equations ===\")\n",
                "show_tokenization(\"E = mc^2 where m is mass and c is the speed of light.\")\n",
                "\n",
                "print(\"=== Control Systems ===\")\n",
                "show_tokenization(\"The PID controller adjusts the feedback loop gain.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Vocabulary Analysis\n",
                "\n",
                "Let's peek at what tokens we learned."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vocab = tokenizer.get_vocab()\n",
                "\n",
                "# Sort by token ID to see the order they were added\n",
                "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
                "\n",
                "print(\"First 20 tokens (special + single chars):\")\n",
                "for token, idx in sorted_vocab[:20]:\n",
                "    print(f\"  {idx:5d}: '{token}'\")\n",
                "\n",
                "print(\"\\n...\\n\")\n",
                "\n",
                "print(\"Sample of learned merge tokens (around ID 5000):\")\n",
                "for token, idx in sorted_vocab[5000:5020]:\n",
                "    print(f\"  {idx:5d}: '{token}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Look for technical terms we'd want as single tokens\n",
                "technical_terms = ['system', 'control', 'reactor', 'pressure', 'temperature',\n",
                "                   'feedback', 'stability', 'equation', 'function', 'transfer']\n",
                "\n",
                "print(\"Technical terms in vocabulary:\")\n",
                "for term in technical_terms:\n",
                "    if term in vocab:\n",
                "        print(f\"  âœ“ '{term}' is a single token (ID: {vocab[term]})\")\n",
                "    else:\n",
                "        # Show how it gets split\n",
                "        tokens = tokenizer.encode(term).tokens\n",
                "        print(f\"  âœ— '{term}' splits into: {tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Save the Tokenizer\n",
                "\n",
                "We'll save this and use it for all training phases. **Never retrain the tokenizer** - it needs to stay consistent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to disk\n",
                "tokenizer_path = TOKENIZER_DIR / \"tokenizer.json\"\n",
                "tokenizer.save(str(tokenizer_path))\n",
                "\n",
                "print(f\"âœ“ Tokenizer saved to: {tokenizer_path.resolve()}\")\n",
                "\n",
                "# Verify we can load it back\n",
                "loaded_tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
                "test = loaded_tokenizer.encode(\"This is a test.\")\n",
                "print(f\"âœ“ Reload test passed: {test.tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Tokenization Statistics\n",
                "\n",
                "Finally, let's see how efficiently our tokenizer compresses the training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a sample of the base stream\n",
                "with open(DATA_DIR / \"base_stream.txt\", 'r', encoding='utf-8') as f:\n",
                "    sample = f.read(100000)  # First 100KB\n",
                "\n",
                "encoding = tokenizer.encode(sample)\n",
                "\n",
                "chars = len(sample)\n",
                "tokens = len(encoding.tokens)\n",
                "ratio = chars / tokens\n",
                "\n",
                "print(\"Tokenization efficiency (100KB sample):\")\n",
                "print(f\"  Characters: {chars:,}\")\n",
                "print(f\"  Tokens: {tokens:,}\")\n",
                "print(f\"  Chars per token: {ratio:.2f}\")\n",
                "print(f\"\\n  (Higher is better - fewer tokens for same text)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "We now have a trained BPE tokenizer with:\n",
                "- **32K vocabulary** learned from pre-1986 text\n",
                "- **Special tokens:** `<PAD>`, `<UNK>`, `<EOS>`, `<BOS>`\n",
                "- Saved to `tokenizer/tokenizer.json`\n",
                "\n",
                "**Next:** In notebook 03, we'll build the model architecture using this tokenizer."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}