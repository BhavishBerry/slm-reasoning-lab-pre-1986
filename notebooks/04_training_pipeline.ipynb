{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Training Pipeline: 3-Phase Training Methodology\n",
                "\n",
                "This notebook implements the complete training pipeline for our SLM. Training happens in three phases:\n",
                "\n",
                "1. **Phase A: Base Pretraining** - Learn language at 2k context\n",
                "2. **Phase B: Context Extension** - Extend to 4k-5k with RoPE scaling\n",
                "3. **Phase C: Domain Fine-Tuning** - Shape reasoning (no labels/answers)\n",
                "\n",
                "Each phase builds on the previous checkpoint."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tokenizers import Tokenizer\n",
                "from pathlib import Path\n",
                "import json\n",
                "import math\n",
                "import time\n",
                "from dataclasses import dataclass\n",
                "\n",
                "# Check device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Training on: {device}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "DATA_DIR = Path(\"../data/pre1986_training_streams_v1_FINAL\")\n",
                "TOKENIZER_PATH = Path(\"../tokenizer/tokenizer.json\")\n",
                "CHECKPOINT_DIR = Path(\"../checkpoints\")\n",
                "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
                "print(f\"âœ“ Loaded tokenizer with vocab size: {tokenizer.get_vocab_size()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Dataset: Text Streaming\n",
                "\n",
                "We create chunks dynamically from our text streams. Documents are separated by `<EOS>` tokens."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset that chunks text into fixed-length sequences.\n",
                "    \n",
                "    We tokenize once and store the token IDs, then serve\n",
                "    random chunks during training.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, file_path: Path, tokenizer: Tokenizer, seq_len: int):\n",
                "        self.seq_len = seq_len\n",
                "        \n",
                "        # Load and tokenize the entire file\n",
                "        print(f\"Loading {file_path.name}...\")\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "        \n",
                "        print(f\"Tokenizing ({len(text):,} chars)...\")\n",
                "        encoding = tokenizer.encode(text)\n",
                "        self.tokens = torch.tensor(encoding.ids, dtype=torch.long)\n",
                "        \n",
                "        # Number of complete chunks we can make\n",
                "        self.n_chunks = (len(self.tokens) - 1) // seq_len\n",
                "        print(f\"Created {self.n_chunks:,} chunks of length {seq_len}\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return self.n_chunks\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Get a chunk of tokens\n",
                "        start = idx * self.seq_len\n",
                "        end = start + self.seq_len + 1  # +1 for target\n",
                "        \n",
                "        chunk = self.tokens[start:end]\n",
                "        \n",
                "        # Input and target (shifted by 1)\n",
                "        x = chunk[:-1]  # Input tokens\n",
                "        y = chunk[1:]   # Target tokens (predict next token)\n",
                "        \n",
                "        return x, y\n",
                "\n",
                "# Quick test\n",
                "test_dataset = TextDataset(DATA_DIR / \"base_stream.txt\", tokenizer, seq_len=128)\n",
                "x, y = test_dataset[0]\n",
                "print(f\"\\nSample batch: input shape={x.shape}, target shape={y.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"All training hyperparameters in one place.\"\"\"\n",
                "    \n",
                "    # Batch settings\n",
                "    batch_size: int = 8\n",
                "    gradient_accumulation_steps: int = 4  # Effective batch = 32\n",
                "    \n",
                "    # Learning rate\n",
                "    learning_rate: float = 3e-4\n",
                "    weight_decay: float = 0.1\n",
                "    warmup_steps: int = 1000\n",
                "    \n",
                "    # Training duration\n",
                "    max_steps: int = 50000\n",
                "    eval_interval: int = 500\n",
                "    save_interval: int = 2000\n",
                "    \n",
                "    # Context length (varies by phase)\n",
                "    seq_len: int = 2048\n",
                "\n",
                "train_config = TrainingConfig()\n",
                "print(f\"Effective batch size: {train_config.batch_size * train_config.gradient_accumulation_steps}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Learning Rate Scheduler\n",
                "\n",
                "We use warmup followed by cosine decay - standard practice for transformer training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_lr(step: int, config: TrainingConfig) -> float:\n",
                "    \"\"\"\n",
                "    Learning rate schedule: linear warmup then cosine decay.\n",
                "    \"\"\"\n",
                "    # Warmup phase\n",
                "    if step < config.warmup_steps:\n",
                "        return config.learning_rate * step / config.warmup_steps\n",
                "    \n",
                "    # Cosine decay phase\n",
                "    decay_ratio = (step - config.warmup_steps) / (config.max_steps - config.warmup_steps)\n",
                "    decay_ratio = min(decay_ratio, 1.0)\n",
                "    \n",
                "    # Cosine decay to 10% of peak LR\n",
                "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
                "    return config.learning_rate * 0.1 + coeff * (config.learning_rate * 0.9)\n",
                "\n",
                "# Visualize the schedule\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "steps = range(train_config.max_steps)\n",
                "lrs = [get_lr(s, train_config) for s in steps]\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(steps, lrs)\n",
                "plt.xlabel('Step')\n",
                "plt.ylabel('Learning Rate')\n",
                "plt.title('LR Schedule: Warmup + Cosine Decay')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Loop\n",
                "\n",
                "The core training function - handles gradient accumulation, logging, and checkpointing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_step(model, batch, optimizer, config, step):\n",
                "    \"\"\"\n",
                "    Single training step with gradient accumulation.\n",
                "    \"\"\"\n",
                "    x, y = batch\n",
                "    x, y = x.to(device), y.to(device)\n",
                "    \n",
                "    # Forward pass\n",
                "    logits = model(x)  # [batch, seq, vocab]\n",
                "    \n",
                "    # Cross-entropy loss (flatten for computation)\n",
                "    loss = F.cross_entropy(\n",
                "        logits.view(-1, logits.size(-1)),\n",
                "        y.view(-1)\n",
                "    )\n",
                "    \n",
                "    # Scale loss for gradient accumulation\n",
                "    loss = loss / config.gradient_accumulation_steps\n",
                "    loss.backward()\n",
                "    \n",
                "    return loss.item() * config.gradient_accumulation_steps  # Return unscaled loss\n",
                "\n",
                "\n",
                "def train(model, train_loader, config, checkpoint_name=\"model\"):\n",
                "    \"\"\"\n",
                "    Main training loop.\n",
                "    \"\"\"\n",
                "    model.to(device)\n",
                "    model.train()\n",
                "    \n",
                "    # Optimizer\n",
                "    optimizer = torch.optim.AdamW(\n",
                "        model.parameters(),\n",
                "        lr=config.learning_rate,\n",
                "        weight_decay=config.weight_decay,\n",
                "        betas=(0.9, 0.95)\n",
                "    )\n",
                "    \n",
                "    # Training state\n",
                "    step = 0\n",
                "    running_loss = 0.0\n",
                "    start_time = time.time()\n",
                "    \n",
                "    print(f\"Starting training for {config.max_steps} steps...\")\n",
                "    print(f\"Batch size: {config.batch_size}, Grad accum: {config.gradient_accumulation_steps}\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    data_iter = iter(train_loader)\n",
                "    \n",
                "    while step < config.max_steps:\n",
                "        # Update learning rate\n",
                "        lr = get_lr(step, config)\n",
                "        for param_group in optimizer.param_groups:\n",
                "            param_group['lr'] = lr\n",
                "        \n",
                "        # Gradient accumulation loop\n",
                "        optimizer.zero_grad()\n",
                "        accum_loss = 0.0\n",
                "        \n",
                "        for micro_step in range(config.gradient_accumulation_steps):\n",
                "            try:\n",
                "                batch = next(data_iter)\n",
                "            except StopIteration:\n",
                "                # Restart from beginning if we run out of data\n",
                "                data_iter = iter(train_loader)\n",
                "                batch = next(data_iter)\n",
                "            \n",
                "            loss = train_step(model, batch, optimizer, config, step)\n",
                "            accum_loss += loss\n",
                "        \n",
                "        # Clip gradients (prevents exploding gradients)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        \n",
                "        # Update weights\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += accum_loss\n",
                "        step += 1\n",
                "        \n",
                "        # Logging\n",
                "        if step % config.eval_interval == 0:\n",
                "            avg_loss = running_loss / config.eval_interval\n",
                "            elapsed = time.time() - start_time\n",
                "            steps_per_sec = step / elapsed\n",
                "            \n",
                "            print(f\"Step {step:6d} | Loss: {avg_loss:.4f} | LR: {lr:.2e} | {steps_per_sec:.1f} steps/s\")\n",
                "            running_loss = 0.0\n",
                "        \n",
                "        # Save checkpoint\n",
                "        if step % config.save_interval == 0:\n",
                "            save_path = CHECKPOINT_DIR / f\"{checkpoint_name}_step{step}.pt\"\n",
                "            torch.save({\n",
                "                'step': step,\n",
                "                'model_state_dict': model.state_dict(),\n",
                "                'optimizer_state_dict': optimizer.state_dict(),\n",
                "                'loss': avg_loss\n",
                "            }, save_path)\n",
                "            print(f\"  â†’ Saved checkpoint: {save_path.name}\")\n",
                "    \n",
                "    print(\"-\" * 60)\n",
                "    print(f\"Training complete! Final loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Phase A: Base Pretraining\n",
                "\n",
                "Train on `base_stream.txt` at 2k context with full attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import model from previous notebook (or copy the code here)\n",
                "# For now, we'll reference the model we built\n",
                "\n",
                "# Load model config\n",
                "with open('../configs/model_config.json', 'r') as f:\n",
                "    model_config_dict = json.load(f)\n",
                "\n",
                "print(\"Model configuration:\")\n",
                "for k, v in model_config_dict.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase A configuration\n",
                "phase_a_config = TrainingConfig(\n",
                "    batch_size=8,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=3e-4,\n",
                "    warmup_steps=1000,\n",
                "    max_steps=50000,\n",
                "    seq_len=2048  # 2k context for Phase A\n",
                ")\n",
                "\n",
                "print(\"Phase A: Base Pretraining\")\n",
                "print(f\"  Context length: {phase_a_config.seq_len}\")\n",
                "print(f\"  Data: base_stream.txt\")\n",
                "print(f\"  Attention: Full\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NOTE: This cell would actually run training - commented out for safety\n",
                "# Uncomment to train\n",
                "\n",
                "'''\n",
                "# Create dataset and dataloader\n",
                "train_dataset = TextDataset(\n",
                "    DATA_DIR / \"base_stream.txt\",\n",
                "    tokenizer,\n",
                "    phase_a_config.seq_len\n",
                ")\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=phase_a_config.batch_size,\n",
                "    shuffle=True,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "# Create model (import from notebook 03)\n",
                "from model import SLM, ModelConfig  # You'd need to save model.py from notebook 03\n",
                "\n",
                "model_config = ModelConfig(\n",
                "    use_block_local=False,  # Full attention for Phase A\n",
                "    max_seq_len=2048\n",
                ")\n",
                "model = SLM(model_config)\n",
                "\n",
                "# Train!\n",
                "model = train(model, train_loader, phase_a_config, checkpoint_name=\"phase_a\")\n",
                "'''\n",
                "\n",
                "print(\"Phase A training code ready - uncomment to run\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Phase B: Context Extension\n",
                "\n",
                "Extend from 2k to 5k context using RoPE scaling and block-local attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase B configuration\n",
                "phase_b_config = TrainingConfig(\n",
                "    batch_size=4,  # Smaller batch - longer sequences use more memory\n",
                "    gradient_accumulation_steps=8,\n",
                "    learning_rate=1e-4,  # Lower LR for fine-tuning\n",
                "    warmup_steps=500,\n",
                "    max_steps=10000,\n",
                "    seq_len=4096  # Extended context\n",
                ")\n",
                "\n",
                "print(\"Phase B: Context Extension\")\n",
                "print(f\"  Context length: {phase_b_config.seq_len}\")\n",
                "print(f\"  Data: base_stream.txt (same distribution)\")\n",
                "print(f\"  Attention: Block-local (512 token blocks)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_rope_scaling(model, scale_factor: float = 2.0):\n",
                "    \"\"\"\n",
                "    Scale RoPE frequencies for context extension.\n",
                "    \n",
                "    Linear scaling: divide frequencies by scale_factor.\n",
                "    This lets the model handle positions it hasn't seen before.\n",
                "    \"\"\"\n",
                "    for block in model.blocks:\n",
                "        if hasattr(block.attention, 'rotary'):\n",
                "            # Scale the inverse frequencies\n",
                "            block.attention.rotary.inv_freq = block.attention.rotary.inv_freq / scale_factor\n",
                "            # Rebuild the cache for the new context length\n",
                "            block.attention.rotary._build_cache(model.config.max_seq_len)\n",
                "    \n",
                "    print(f\"Applied RoPE scaling with factor {scale_factor}\")\n",
                "\n",
                "# This would be applied when loading a Phase A checkpoint for Phase B\n",
                "print(\"RoPE scaling function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Phase C: Domain Fine-Tuning\n",
                "\n",
                "Fine-tune on control systems, nuclear, and reliability texts.\n",
                "\n",
                "**Key principle:** No labels, no answers, no chain-of-thought forcing. We're shaping *how* the model reasons, not *what* conclusions it reaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiStreamDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset that samples from multiple fine-tuning streams.\n",
                "    \n",
                "    We want the model to see all domains, so we mix them.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, file_paths: list, tokenizer: Tokenizer, seq_len: int):\n",
                "        self.seq_len = seq_len\n",
                "        self.all_tokens = []\n",
                "        \n",
                "        for path in file_paths:\n",
                "            print(f\"Loading {path.name}...\")\n",
                "            with open(path, 'r', encoding='utf-8') as f:\n",
                "                text = f.read()\n",
                "            \n",
                "            encoding = tokenizer.encode(text)\n",
                "            tokens = torch.tensor(encoding.ids, dtype=torch.long)\n",
                "            self.all_tokens.append(tokens)\n",
                "            print(f\"  â†’ {len(tokens):,} tokens\")\n",
                "        \n",
                "        # Concatenate all streams\n",
                "        self.tokens = torch.cat(self.all_tokens)\n",
                "        self.n_chunks = (len(self.tokens) - 1) // seq_len\n",
                "        print(f\"\\nTotal: {len(self.tokens):,} tokens, {self.n_chunks:,} chunks\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return self.n_chunks\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        start = idx * self.seq_len\n",
                "        end = start + self.seq_len + 1\n",
                "        chunk = self.tokens[start:end]\n",
                "        return chunk[:-1], chunk[1:]\n",
                "\n",
                "# Fine-tuning files\n",
                "finetune_files = [\n",
                "    DATA_DIR / \"finetune_control.txt\",\n",
                "    DATA_DIR / \"finetune_nuclear.txt\",\n",
                "    DATA_DIR / \"finetune_reliability.txt\"\n",
                "]\n",
                "\n",
                "print(\"Fine-tuning streams:\")\n",
                "for f in finetune_files:\n",
                "    print(f\"  - {f.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase C configuration\n",
                "phase_c_config = TrainingConfig(\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=5e-5,  # Even lower LR for fine-tuning\n",
                "    warmup_steps=200,\n",
                "    max_steps=5000,\n",
                "    seq_len=4096  # Keep extended context\n",
                ")\n",
                "\n",
                "print(\"Phase C: Domain Fine-Tuning\")\n",
                "print(f\"  Context length: {phase_c_config.seq_len}\")\n",
                "print(f\"  Data: control + nuclear + reliability streams\")\n",
                "print(f\"  Attention: Block-local\")\n",
                "print(f\"\\nRemember: NO labels, NO answers - just exposure to domain text\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Putting It All Together\n",
                "\n",
                "Here's the complete training pipeline as a script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def full_training_pipeline():\n",
                "    \"\"\"\n",
                "    Complete 3-phase training pipeline.\n",
                "    \n",
                "    This would take a long time to run - it's here as a reference.\n",
                "    \"\"\"\n",
                "    \n",
                "    # ============ PHASE A ============\n",
                "    print(\"=\" * 60)\n",
                "    print(\"PHASE A: Base Pretraining\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Create model with full attention\n",
                "    model_config = ModelConfig(use_block_local=False, max_seq_len=2048)\n",
                "    model = SLM(model_config)\n",
                "    \n",
                "    # Create dataset\n",
                "    train_dataset = TextDataset(DATA_DIR / \"base_stream.txt\", tokenizer, 2048)\n",
                "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
                "    \n",
                "    # Train Phase A\n",
                "    model = train(model, train_loader, phase_a_config, \"phase_a\")\n",
                "    \n",
                "    # ============ PHASE B ============\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PHASE B: Context Extension\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Switch to block-local attention and extend context\n",
                "    model.config.use_block_local = True\n",
                "    model.config.max_seq_len = 4096\n",
                "    apply_rope_scaling(model, scale_factor=2.0)\n",
                "    \n",
                "    # Same data, longer sequences\n",
                "    train_dataset = TextDataset(DATA_DIR / \"base_stream.txt\", tokenizer, 4096)\n",
                "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
                "    \n",
                "    # Train Phase B\n",
                "    model = train(model, train_loader, phase_b_config, \"phase_b\")\n",
                "    \n",
                "    # ============ PHASE C ============\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PHASE C: Domain Fine-Tuning\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Fine-tuning data\n",
                "    train_dataset = MultiStreamDataset(finetune_files, tokenizer, 4096)\n",
                "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
                "    \n",
                "    # Train Phase C\n",
                "    model = train(model, train_loader, phase_c_config, \"phase_c_final\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"TRAINING COMPLETE!\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    return model\n",
                "\n",
                "print(\"Full training pipeline defined\")\n",
                "print(\"Run full_training_pipeline() to start (this will take many hours)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "We've set up the complete training infrastructure:\n",
                "\n",
                "| Phase | Context | Attention | Data | LR |\n",
                "|-------|---------|-----------|------|----|\n",
                "| A | 2048 | Full | base_stream | 3e-4 |\n",
                "| B | 4096 | Block-local | base_stream | 1e-4 |\n",
                "| C | 4096 | Block-local | finetune_* | 5e-5 |\n",
                "\n",
                "Each phase builds on the previous checkpoint, progressively extending context and shaping reasoning.\n",
                "\n",
                "**Next:** In notebook 05, we'll evaluate the trained model with zero-shot prompts."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}