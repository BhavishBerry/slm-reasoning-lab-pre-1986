{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Data Exploration: Pre-1986 Training Streams\n",
    "\n",
    "This notebook explores the training data for our from-scratch Small Language Model (SLM). We're building a 300M parameter model trained **exclusively on pre-1986 knowledge** for counterfactual safety reasoning.\n",
    "\n",
    "**Goal:** Understand what we're working with before we start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports\n",
    "\n",
    "Just the basics - nothing fancy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# We'll use matplotlib for some simple visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Where our training data lives\n",
    "DATA_DIR = Path(\"../data/pre1986_training_streams_v1_FINAL\")\n",
    "\n",
    "# Quick sanity check\n",
    "if DATA_DIR.exists():\n",
    "    print(f\"âœ“ Data directory found: {DATA_DIR.resolve()}\")\n",
    "    print(f\"  Files: {list(DATA_DIR.glob('*.txt'))}\")\n",
    "else:\n",
    "    print(\"âœ— Data directory not found - check your path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Training Streams\n",
    "\n",
    "Our dataset consists of **4 text streams**, each serving a different purpose:\n",
    "\n",
    "| File | Purpose | Training Phase |\n",
    "|------|---------|----------------|\n",
    "| `base_stream.txt` | General knowledge (books, textbooks, essays) | Phase A: Pretraining |\n",
    "| `finetune_control.txt` | Control systems theory | Phase C: Fine-tuning |\n",
    "| `finetune_nuclear.txt` | Nuclear engineering concepts | Phase C: Fine-tuning |\n",
    "| `finetune_reliability.txt` | System reliability analysis | Phase C: Fine-tuning |\n",
    "\n",
    "Let's see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_stats(filepath):\n",
    "    \"\"\"Get basic stats about a text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count some basic things\n",
    "    lines = content.split('\\n')\n",
    "    words = content.split()\n",
    "    chars = len(content)\n",
    "    \n",
    "    return {\n",
    "        'size_mb': os.path.getsize(filepath) / (1024 * 1024),\n",
    "        'lines': len(lines),\n",
    "        'words': len(words),\n",
    "        'chars': chars,\n",
    "        'sample': content[:500]  # First 500 chars as a preview\n",
    "    }\n",
    "\n",
    "# Gather stats for all our files\n",
    "files = ['base_stream.txt', 'finetune_control.txt', \n",
    "         'finetune_nuclear.txt', 'finetune_reliability.txt']\n",
    "\n",
    "stats = {}\n",
    "for fname in files:\n",
    "    fpath = DATA_DIR / fname\n",
    "    if fpath.exists():\n",
    "        stats[fname] = get_file_stats(fpath)\n",
    "        print(f\"{fname}:\")\n",
    "        print(f\"  Size: {stats[fname]['size_mb']:.2f} MB\")\n",
    "        print(f\"  Words: {stats[fname]['words']:,}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visualizing the Data Distribution\n",
    "\n",
    "Let's see how the data is split between base pretraining and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bar chart of file sizes\n",
    "names = list(stats.keys())\n",
    "sizes = [stats[n]['size_mb'] for n in names]\n",
    "\n",
    "# Make it look decent\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']  # Nice colors\n",
    "bars = plt.bar(names, sizes, color=colors)\n",
    "\n",
    "plt.ylabel('Size (MB)')\n",
    "plt.title('Training Data Size by Stream')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "# Add size labels on top of bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{size:.1f} MB', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# The base stream is MUCH larger - that's intentional\n",
    "base_pct = stats['base_stream.txt']['size_mb'] / sum(sizes) * 100\n",
    "print(f\"\\nBase pretraining data is {base_pct:.1f}% of total - this is by design.\")\n",
    "print(\"Fine-tuning is meant to *shape* reasoning, not add bulk knowledge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Peeking at the Content\n",
    "\n",
    "Let's look at actual samples from each stream to understand what the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(filename, start_pos=0, length=1000):\n",
    "    \"\"\"Show a sample from a file starting at a given position.\"\"\"\n",
    "    with open(DATA_DIR / filename, 'r', encoding='utf-8') as f:\n",
    "        f.seek(start_pos)\n",
    "        sample = f.read(length)\n",
    "    \n",
    "    print(f\"=== {filename} (starting at byte {start_pos}) ===\")\n",
    "    print(sample)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show samples from each file\n",
    "# Starting from different positions to see variety\n",
    "show_sample('base_stream.txt', start_pos=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning data - control systems\n",
    "show_sample('finetune_control.txt', start_pos=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning data - nuclear engineering\n",
    "show_sample('finetune_nuclear.txt', start_pos=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Document Structure: EOS Markers\n",
    "\n",
    "Our data uses `<EOS>` (End Of Sequence) markers to separate documents. This is important because:\n",
    "- The model learns that `<EOS>` means \"this thought is complete\"\n",
    "- We don't want the model to blend unrelated documents together\n",
    "\n",
    "Let's count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_eos_markers(filename):\n",
    "    \"\"\"Count how many documents are in a file.\"\"\"\n",
    "    with open(DATA_DIR / filename, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content.count('<EOS>')\n",
    "\n",
    "print(\"Document counts (based on <EOS> markers):\")\n",
    "print(\"-\" * 40)\n",
    "for fname in files:\n",
    "    count = count_eos_markers(fname)\n",
    "    print(f\"{fname}: {count:,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Character & Word Distribution\n",
    "\n",
    "Let's look at what characters appear in our data. This helps us understand:\n",
    "- Is there any weird encoding issues?\n",
    "- What special characters do we need to handle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze base_stream since it's the largest\n",
    "with open(DATA_DIR / 'base_stream.txt', 'r', encoding='utf-8') as f:\n",
    "    base_content = f.read()\n",
    "\n",
    "# Count characters\n",
    "char_counts = Counter(base_content)\n",
    "\n",
    "# Show top 30 most common characters\n",
    "print(\"Top 30 most common characters in base_stream.txt:\")\n",
    "print(\"-\" * 50)\n",
    "for char, count in char_counts.most_common(30):\n",
    "    # Make whitespace visible\n",
    "    display_char = repr(char) if char in '\\n\\t\\r ' else char\n",
    "    print(f\"  {display_char}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any unusual unicode characters\n",
    "unusual = {char: count for char, count in char_counts.items() \n",
    "           if ord(char) > 127}  # Non-ASCII\n",
    "\n",
    "if unusual:\n",
    "    print(\"Non-ASCII characters found:\")\n",
    "    for char, count in sorted(unusual.items(), key=lambda x: -x[1])[:20]:\n",
    "        print(f\"  U+{ord(char):04X} '{char}': {count:,}\")\n",
    "else:\n",
    "    print(\"âœ“ All characters are ASCII - clean data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Key Takeaways\n",
    "\n",
    "Before moving on to tokenization, here's what we learned:\n",
    "\n",
    "1. **Data Scale:** Base pretraining stream is ~50MB, fine-tuning streams are much smaller (~2.5MB combined)\n",
    "2. **Structure:** Documents are separated by `<EOS>` markers\n",
    "3. **Content Quality:** Pre-1986 scientific and engineering text - exactly what we need\n",
    "4. **Encoding:** UTF-8 encoded, mostly ASCII with some special characters for equations\n",
    "\n",
    "**Next:** In notebook 02, we'll train a BPE tokenizer on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats to remember\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "total_mb = sum(s['size_mb'] for s in stats.values())\n",
    "total_words = sum(s['words'] for s in stats.values())\n",
    "print(f\"Total data size: {total_mb:.2f} MB\")\n",
    "print(f\"Total word count: {total_words:,}\")\n",
    "print(f\"Files: {len(stats)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
